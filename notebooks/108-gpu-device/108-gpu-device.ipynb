{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85bb873e-562a-4d60-9536-b6b88aecb5c2",
   "metadata": {},
   "source": [
    "# Working with GPUs in OpenVINO™\n",
    "\n",
    "This tutorial provides a high-level overview of working with Intel GPUs in OpenVINO. It shows users how to use Query Device to list system GPUs and check their properties, and it explains some of the key properties. It shows how to compile a model on GPU with performance hints and how to use multiple GPUs using MULTI or CUMULATIVE_THROUGHPUT. \n",
    "\n",
    "The tutorial also shows example commands for benchmark_app that users can run to compare GPU performance in different configurations. It also provides code for a basic end-to-end application that compiles a model on GPU and uses it to run inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7253ed-d8a1-4475-8e83-263336639157",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f9f13-63a4-4b2d-95e3-9a82c1e9cebd",
   "metadata": {},
   "source": [
    "1. Background and context on how GPUs are used to speed up inference\n",
    "2. Introduce OpenVINO’s ability to run inference with GPUs\n",
    "3. How to configure OpenVINO to work with GPUs (link to Configuration for GPU with OpenVINO page)\n",
    "\n",
    "Originally, graphic processing units (GPUs) began as specialized chips developed to accelerate the rendering of computer graphics. In contrast to CPUs, which have few but powerful cores, GPUs have many more specialized cores, making them ideal for workloads that can be parallelized into simpler tasks. Nowadays, one such workload is deep learning, in which GPUs shine the brightest when training several neural network layers or on massive sets of certain data, like 2D images. For inference though, GPUs tend to not be a requirement but depending on the application, the size of the network and the amount of data to be processed that need may arise. In any case, OpenVINO already has the ability to do so thanks to their [GPU plugin](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_GPU.html), which can be as easy as specifying the GPU device when compiling a model, as we will see later on. To get started, make sure to follow the [instructions to install OpenVINO](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/download.html) and keep reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f35f17-5209-43d6-b3b5-db35efb1f42e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Checking GPUs with Query Device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3ce23-2b04-4014-9eeb-c2b8309eb103",
   "metadata": {},
   "source": [
    "1. List GPUs with ie.get_available_devices\n",
    "2. Check properties with ie.get_property\n",
    "3. Brief descriptions of key properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c37c2-b24b-4c9f-9ab8-d3a60b8393ec",
   "metadata": {},
   "source": [
    "In this section we will see how to list the available GPUs and check their properties. Some of the key properties will also be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e91abc-6118-4532-9817-6ef44c51b0c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### List GPUs with core.get_available_devices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b6152-3915-415a-987b-c30935462007",
   "metadata": {},
   "source": [
    "OpenVINO Runtime provides the `available_devices` method for checking which devices are avaiable for inferencing. The following code will output a list of compatible OpenVINO devices, in which our Intel GPUs should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8260bef-63c4-45f3-9ffd-4cc3ac892680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "core.available_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e266c79-7bb9-4907-9204-87b688087cbf",
   "metadata": {},
   "source": [
    "If the GPUs are installed correctly in the system and still don't appear in the list, follow the steps described [here](https://docs.openvino.ai/latest/openvino_docs_install_guides_configurations_for_intel_gpu.html) to configure your GPU drivers to work with OpenVINO. Once we have the GPUs working with OpenVINO, we can proceed with the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0491c07-0b6d-483f-a4d6-d3f7b5ec0c81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check properties with core.get_property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54399a50-31cf-48bd-bf98-6dea0c0b1b20",
   "metadata": {},
   "source": [
    "To get information about our GPUs, we can use device properties. In OpenVINO, devices have properties that describe their characteristics and configuration. Each property has a name and associated value that can be queried with the `get_property` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7509b6-67e3-46fb-bb23-5f46c58b0fc1",
   "metadata": {},
   "source": [
    "To get the value of a property, such as the device name, we can use the `get_property` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacdd5be-5d75-41d5-a51b-a376cb063b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "core.get_property(\"GPU\", \"FULL_DEVICE_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3129a-129f-49aa-aba0-71ae1e892ada",
   "metadata": {},
   "source": [
    "Each device also has a specific property called `SUPPORTED_PROPERTIES`, that allows us to see all the available properties in the device. We can check the value for each property by simply looping through the dictionary returned by `core.get_property(\"GPU\", \"SUPPORTED_PROPERTIES\")` and then querying for that property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb073e8-c997-40db-92fa-93334731285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"GPU\"\n",
    "\n",
    "print(f\"{device} SUPPORTED_PROPERTIES:\\n\")\n",
    "supported_properties = core.get_property(device, \"SUPPORTED_PROPERTIES\")\n",
    "indent = len(max(supported_properties, key=len))\n",
    "\n",
    "for property_key in supported_properties:\n",
    "    if property_key not in ('SUPPORTED_METRICS', 'SUPPORTED_CONFIG_KEYS', 'SUPPORTED_PROPERTIES'):\n",
    "        try:\n",
    "            property_val = core.get_property(device, property_key)\n",
    "        except TypeError:\n",
    "            property_val = 'UNSUPPORTED TYPE'\n",
    "        print(f\"{property_key:<{indent}}: {property_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88546b71-2ae8-4519-b9b7-a54f444e204f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Brief descriptions of key properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524096f-cc77-4dca-9e52-e5a3c2c0523b",
   "metadata": {},
   "source": [
    "Each device has several properties as seen in the last command. Some of the key properties are:\n",
    "\n",
    "* `FULL_DEVICE_NAME` - The product name of the GPU and whether it is an integrated or discrete GPU (iGPU or dGPU).\n",
    "* `OPTIMIZATION_CAPABILITIES` - The model data types (INT8, FP16, FP32, etc) that are supported by this GPU.\n",
    "* `GPU_EXECUTION_UNITS_COUNT` - The execution cores available in the GPU's architecture, which is a relative measure of the GPU's processing power.\n",
    "* `RANGE_FOR_STREAMS` - The number of processing streams available on the GPU that can be used to execute parallel inference requests. When compiling a model in LATENCY or THROUGHPUT mode, OpenVINO will automatically select the best number of streams for low latency or high throughput.\n",
    "* `PERFORMANCE_HINT` - A high-level way to tune the device for a specific performance metric, such as latency or throughput, without worrying about device-specific settings.\n",
    "\n",
    "To learn more about devices and properties, see the [Query Device Properties](https://docs.openvino.ai/latest/openvino_docs_OV_UG_query_api.html) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131bd0f9-9b1e-4fc1-ad09-3960e1d6c50f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compiling a Model on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e33c10-f8cc-494d-bfa7-567787590946",
   "metadata": {},
   "source": [
    "1. Compile with default configuration (ie.compile_model(model, “GPU”)\n",
    "2. Throughput and latency performance hints\n",
    "3. Using multiple GPUs with multi-device and cumulative throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea15935-2373-4029-8bc0-998558a6defe",
   "metadata": {},
   "source": [
    "We now know how to list the GPUs in our system and check their properties, but how do we actually use one? OpenVINO provides a [GPU plugin](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_GPU.html) that allows us to easily compile models and run them on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc043aa-8729-45bc-9e98-6b3daea9f271",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile with default configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999d5e8-3429-4e2b-a845-c5569ec1895f",
   "metadata": {},
   "source": [
    "To compile our model, we first need to read it using the `read_model` method. Then, we can use the `compile_model` method and specify the name of the device we want to compile the model on (in this case, \"GPU\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7d00e-ebe5-456d-8643-112e9d8c860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = core.read_model(model=\"model/v3-small_224_1.0_float.xml\")\n",
    "compiled_model = core.compile_model(model, \"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c58745-69c3-4034-b221-bc4cf8a5b27b",
   "metadata": {},
   "source": [
    "If you have multiple GPUs in the system, you can specify which one to use by using \"GPU.0\", \"GPU.1\", etc. Any of the device names returned by `core.available_devices` are valid device specifiers. You may also use \"AUTO\", which will automatically select the best device for inferencing (which is often the GPU). To learn more about AUTO plugin, visit the [Automatic Device Selection](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_AUTO.html) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa916d7-4626-4f54-9b34-954eca19aafb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Throughput and latency performance hints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cbc96-8832-4ed9-9109-30b0efce9da9",
   "metadata": {},
   "source": [
    "To simplify device and pipeline configuration, OpenVINO provides high-lvel performance hints that automatically set the batch size and number of parallel threads to use for inferencing. The \"LATENCY\" performance hint optimizes for fast inferencing times while the \"THROUGHPUT\" performance hint optimizes for high overall bandwith or FPS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077b662-22f3-4c52-9c80-e5ac1309c482",
   "metadata": {},
   "source": [
    "To use the \"LATENCY\" performance hint, add `{\"PERFORMANCE_HINT\": \"LATENCY\"}` when compiling the model as shown below. For GPUs, this automaticallt minimizes the batch size and number of parallel streams such that all of the compute resources can focus on completing a single inference as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4465a003-03ff-497f-94b5-3a194b6e386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model, \"GPU\", {\"PERFORMANCE_HINT\": \"LATENCY\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06589f38-ce35-457f-8395-a4a3f6327ea0",
   "metadata": {},
   "source": [
    "To use the \"THROUGHPUT\" performance hint, add `{\"PERFORMANCE_HINT\": \"THROUGHPUT\"}` when compiling the model. For GPUs, this creates multiple processing streams to efficiently utilize all the execution cores and optimizes the batch size to fill the memory available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd2215-00d7-4838-aa76-040fcee18a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model, \"GPU\", {\"PERFORMANCE_HINT\": \"THROUGHPUT\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a98815-669e-4868-a5f3-7104d6887fb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using multiple GPUs with multi-device and cumulative throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da18381-edfc-4d3b-942a-87d6b0cbd5d3",
   "metadata": {},
   "source": [
    "The latency and throughput hints mentioned above are great and can make a difference when used adequately but they usually use just one device, either due to the [AUTO plugin](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_AUTO.html#how-auto-works) or by manual specification of the device name as we did above. In case we have multiple devices, such as an integrated and discrete GPU, we could use both at the same time to improve the utlization of our resources. In order to do this, OpenVINO provides a virtual device called [MULTI](https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Running_on_multiple_devices.html), which is just a combination of our existent devices that knows how to split inference work between them, leveraging the capabilities of each device.\n",
    "\n",
    "So, as an example, if we want to use both our integrated and discrete GPUs and the CPU at the same time, we can compile our model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e7910-70b8-4971-8b82-0569accbc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model=model, device_name=\"MULTI:GPU.1,GPU.0,CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53791dad-88a5-4198-9013-5ebd2e950d52",
   "metadata": {},
   "source": [
    "Note that we always need to explicitly specify the device list for MULTI to work, as otherwise MULTI does not know which devices are available for inference. However, this is not the only way to use multiple devices in OpenVINO. There is another performance hint called \"CUMULATIVE_THROUGHPUT\" that works similar to MULTI, except it uses the devices automatically selected by AUTO. This way, we don't need to manually specify which devices to use. Here is an example showing how to use \"CUMULATIVE_THROUGHPUT\", equivalent to the MULTI one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134705f4-0e8f-40d1-b9fa-26dfbcc69c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model, \"AUTO\", {\"PERFORMANCE_HINT\": \"CUMULATIVE_THROUGHPUT\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb23aa-fc2d-49f4-9cb9-9dd38de5d9f6",
   "metadata": {},
   "source": [
    "Important note: the “THROUGHPUT”, “MULTI”, and “CUMULATIVE_THROUGHPUT” modes are only applicable to asynchronous inferencing pipelines. The example at the end of this article shows how to set up an asynchronous pipeline that takes advantage of parallelism to increase throughput. To learn more, see [Asynchronous Inferencing](https://docs.openvino.ai/latest/openvino_docs_ie_plugin_dg_async_infer_request.html) in OpenVINO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99351910-c54c-4f55-b1ce-eb74125a9dbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance Comparison with benchmark_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f453eca-41ac-42fc-b77b-9aa8d8e3ab20",
   "metadata": {},
   "source": [
    "1. Commands showing users how to run benchmark_app on GPU with various performance hints\n",
    "2. Show performance results with a basic model (person-detection-0303, perhaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b411e5-5c56-462a-82e3-22ba020f64c1",
   "metadata": {},
   "source": [
    "Given all the different options available when compiling a model, it may be difficult to know which settings work best for a certain application. Thankfully, OpenVINO provides a performance benchmarking tool called `benchmark_app`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df239f8c-2597-46d4-86a0-c23bd1591a33",
   "metadata": {},
   "source": [
    "### Commands showing users how to run benchmark_app on GPU with various performance hints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5921100-0f41-42d3-8f90-703c20cb11bd",
   "metadata": {},
   "source": [
    "The basic syntax of benchmark_app is as follows:\n",
    "\n",
    "```bash\n",
    "benchmark_app -m PATH_TO_MODEL -d TARGET_DEVICE -hint {throughput,cumulative_throughput,latency,none}\n",
    "```\n",
    "where TARGET_DEVICE is any device shown by the `available_devices` method as well as the MULTI and AUTO devices we saw previously, and the value of hint should be one of the values between brackets. \n",
    "\n",
    "Note that benchmark_app only requires the model path to run but both the device and hint arguments will be useful to us. For more advanced usages, the tool itself has other options that can be checked by running `benchmark_app -h` or reading the [docs](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html). The following example shows how to benchmark a simple model using a GPU with a latency focus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c76637-f86c-41d5-8846-6e3946126084",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m model/v3-small_224_1.0_float.xml -d CPU -hint latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81464f73-ef33-47fa-9239-7a817a1ce35b",
   "metadata": {},
   "source": [
    "### Show performance results with a basic model (person-detection-0303, perhaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c27f62b-6053-4a8a-a554-064b04fa2ac7",
   "metadata": {},
   "source": [
    "For completeness, here we list some of the comparisons we may want to do by varying the device and hint used. Note that the actual performance may depend in the hardware used, but overall we should expect a GPU to be better than a CPU, whereas multiple GPUs should be better than a single GPU as long as there is enough work for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a5020-9e9c-48ae-8386-8078e2bf08a2",
   "metadata": {},
   "source": [
    "#### CPU vs GPU with latency hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f487ea-da76-4d6f-9670-9204965ded67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m model/v3-small_224_1.0_float.xml -d CPU -hint latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350075ea-3c4b-467d-bd24-808e51745fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m model/v3-small_224_1.0_float.xml -d GPU -hint latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9432edbe-3078-4534-a0cc-6bfe4bc00b28",
   "metadata": {},
   "source": [
    "#### CPU vs GPU with throughput hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97e8db-f97b-466c-9c2e-f660f2084648",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m model/v3-small_224_1.0_float.xml -d CPU -hint throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586a49c-8eb8-4691-abf1-66ecb2b6334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m model/v3-small_224_1.0_float.xml -d GPU -hint throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e3e87-bdbf-4424-85aa-28c9b0c144f9",
   "metadata": {},
   "source": [
    "#### Single GPU vs Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ebb5f-4e97-42d1-bb64-69dcd13833e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m model/v3-small_224_1.0_float.xml -d GPU.1 -hint throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d5eac-55e9-4850-a191-af96e56855e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m model/v3-small_224_1.0_float.xml -d AUTO:GPU.1,GPU.0 -hint cumulative_throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db3ae8-8c98-46a3-82fa-ab9539c96a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m model/v3-small_224_1.0_float.xml -d MULTI:GPU.1,GPU.0 -hint throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd1496-996d-49d8-80b9-3bfb3dda9729",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Application Using GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bacb918-3668-4e72-92ba-a15db658c369",
   "metadata": {},
   "source": [
    "1. Provide end-to-end sample code for running inference on GPU in a basic application\n",
    "\n",
    "We will now show an end-to-end object detection example running on GPU with the \"THROUGHPUT\" hint.\n",
    "\n",
    "    1. Import necessary packages\n",
    "    2. Download and convert ssdlite-mobilenet-v2 model\n",
    "    3. Read ssd-mobilenet model and compile model on GPU in THROUGHPUT mode\n",
    "    4. Load every frame of a video and resize it to shape expected by model\n",
    "    5. Set up AsyncInferQueue, process every frame of video and store results, time how long it takes\n",
    "    6. Post-process results to print every object detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1686090-2a2e-4ae6-8f22-7ef458ce221b",
   "metadata": {},
   "source": [
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0b948-ec1f-4ab8-9418-103dc8f06c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, Video, display\n",
    "from openvino.runtime import AsyncInferQueue, CompiledModel, Core, InferRequest\n",
    "\n",
    "core = Core()\n",
    "core.available_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24a39d-8819-47a3-9c91-0f80e92649b6",
   "metadata": {},
   "source": [
    "### Download the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a741f-2fc7-430e-b2f7-21d229533447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A directory where the model will be downloaded.\n",
    "base_model_dir = \"model\"\n",
    "\n",
    "# The name of the model from Open Model Zoo\n",
    "model_name = \"ssdlite_mobilenet_v2\"\n",
    "\n",
    "model_path = f\"model/public/{model_name}\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    download_command = f\"omz_downloader \" \\\n",
    "                       f\"--name {model_name} \" \\\n",
    "                       f\"--output_dir {base_model_dir} \" \\\n",
    "                       f\"--cache_dir {base_model_dir}\"\n",
    "    ! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaa8d2-724b-47f5-991d-22bbe6d1c994",
   "metadata": {},
   "source": [
    "### Convert the Model to OpenVINO IR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75b02b-d0a1-422f-b3db-42558ccf5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = \"FP16\"\n",
    "\n",
    "# The output path for the conversion.\n",
    "converted_model_path = f\"model/public/{model_name}/{precision}/{model_name}.xml\"\n",
    "\n",
    "if not os.path.exists(converted_model_path):\n",
    "    convert_command = f\"omz_converter \" \\\n",
    "                      f\"--name {model_name} \" \\\n",
    "                      f\"--download_dir {base_model_dir} \" \\\n",
    "                      f\"--precisions {precision}\"\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfb30e-d6d3-4ed3-8e7e-f1a843ce2ec0",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648ae4f-2c73-4525-ace0-1c71afb18ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = core.read_model(model=converted_model_path)\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\", config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "\n",
    "# Get the input and output nodes.\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "input_keys = list(compiled_model.inputs)\n",
    "print(input_keys)\n",
    "\n",
    "# Get the input size.\n",
    "num, height, width, channels = input_layer.shape\n",
    "print('Model input shape:', num, height, width, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bce3d-647d-46d1-8540-4a2935bc829f",
   "metadata": {},
   "source": [
    "### Load and Preprocess Video Frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee85d9-eab1-4a94-bb2f-d5217fd7cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load video\n",
    "video_file = \"../data/video/Coco Walking in Berkeley.mp4\"\n",
    "video = cv2.VideoCapture(video_file)\n",
    "framebuf = []\n",
    "\n",
    "print('Loading video...')\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        print('Video loaded!')\n",
    "        video.release()\n",
    "        break\n",
    "    \n",
    "    # Preprocess frames - convert them to shape expected by model\n",
    "    input_frame = cv2.resize(src=frame, dsize=(width, height), interpolation=cv2.INTER_AREA)\n",
    "    input_frame = np.expand_dims(input_frame, axis=0)\n",
    "\n",
    "    # Append frame to framebuffer\n",
    "    framebuf.append(input_frame)\n",
    "    \n",
    "\n",
    "print('Frame shape: ', framebuf[0].shape)\n",
    "print('Number of frames: ', len(framebuf))\n",
    "\n",
    "# Show original video file\n",
    "Video(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7003286a-8349-46f6-9d38-c6bb959e892a",
   "metadata": {},
   "source": [
    "### Define Model Output Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b919e-c120-42f9-89eb-ffd7b810c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO classes! (the dataset, not the dog)\n",
    "classes = [\n",
    "    \"background\", \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\",\n",
    "    \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"street sign\", \"stop sign\",\n",
    "    \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\",\n",
    "    \"bear\", \"zebra\", \"giraffe\", \"hat\", \"backpack\", \"umbrella\", \"shoe\", \"eye glasses\",\n",
    "    \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\",\n",
    "    \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\",\n",
    "    \"plate\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
    "    \"couch\", \"potted plant\", \"bed\", \"mirror\", \"dining table\", \"window\", \"desk\", \"toilet\",\n",
    "    \"door\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\",\n",
    "    \"toaster\", \"sink\", \"refrigerator\", \"blender\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "    \"teddy bear\", \"hair drier\", \"toothbrush\", \"hair brush\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad10b97-e32e-4885-af1d-0636e970b707",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set up Asynchronous Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6ed58-8b3f-415e-a733-a7517e200b12",
   "metadata": {},
   "source": [
    "#### Callback Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268e1a4-b530-4b39-b79d-d6d5f988a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_callback(infer_request: InferRequest, results) -> None:\n",
    "    predictions = next(iter(infer_request.results.values()))\n",
    "    results.append(predictions[:10]) # Grab first 10 predictions for this frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f6aba-b3ba-478b-b02f-6b438b575c33",
   "metadata": {},
   "source": [
    "#### Create Async Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8eae2-20a9-43dc-8774-444ad0179136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create async queue with optimal number of infer requests\n",
    "infer_queue = AsyncInferQueue(compiled_model)\n",
    "infer_queue.set_callback(completion_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47ae59-b86d-46c4-b219-e7beccd5f796",
   "metadata": {},
   "source": [
    "### Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f493dd-4cb7-4ae2-b117-7ba41a950d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "start_time = time.time()\n",
    "for i, input_frame in enumerate(framebuf):\n",
    "    infer_queue.start_async({0: input_frame}, results)\n",
    "\n",
    "infer_queue.wait_all()\n",
    "stop_time = time.time()\n",
    "\n",
    "total_time = stop_time - start_time\n",
    "time_per_frame = total_time / len(framebuf)\n",
    "fps = len(framebuf) / total_time\n",
    "print(f'Total time to inference all frames: {total_time:.3f}s')\n",
    "print(f'Time per frame: {time_per_frame:.6f}s ({fps:.3f} FPS)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd90b68-ae14-4fe9-98c7-e8d23146c82c",
   "metadata": {},
   "source": [
    "### Process Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccba843-e40b-43a8-b2b4-8ebd67bc1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set minimum detection threshold\n",
    "min_thresh = .6\n",
    "\n",
    "# Load video\n",
    "video = cv2.VideoCapture(video_file)\n",
    "\n",
    "# Get video parameters\n",
    "frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "fourcc = int(video.get(cv2.CAP_PROP_FOURCC))\n",
    "\n",
    "output = cv2.VideoWriter('output.mp4', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while video.isOpened():\n",
    "    current_frame = int(video.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        print('Video loaded!')\n",
    "        output.release()\n",
    "        video.release()\n",
    "        break\n",
    "\n",
    "    # prediction contains [image_id, label, conf, x_min, y_min, x_max, y_max] according to model\n",
    "    for prediction in np.squeeze(results[current_frame]):\n",
    "        if prediction[2] > min_thresh:\n",
    "            x_min = int(prediction[3] * frame_width)\n",
    "            y_min = int(prediction[4] * frame_height)\n",
    "            x_max = int(prediction[5] * frame_width)\n",
    "            y_max = int(prediction[6] * frame_height)\n",
    "            label = classes[int(prediction[1])]\n",
    "            \n",
    "            # Draw a bounding box with its label above it\n",
    "            image = cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0,255,0), 2)\n",
    "            cv2.putText(image, label, (x_min, y_min - 10), cv2.FONT_ITALIC, 1, (255,0,0), 2)\n",
    "\n",
    "    output.write(frame)\n",
    "\n",
    "# Show output\n",
    "Video(\"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb6bb6-0d1c-4b0b-8a0b-efb5667816ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb4f28-2917-45ca-8fa0-8c670c0348bc",
   "metadata": {},
   "source": [
    "1. GPUs are easy to use with OpenVINO and considerably boost performance\n",
    "2. Links to OpenVINO documentation where readers can learn more\n",
    "\n",
    "In this tutorial we saw how easy it is to use one or more GPUs in OpenVINO, check their properties, and even tailor our model performance through the different performance hints. We also went through a basic object detection application that used a GPU and displayed the detected bounding boxes.\n",
    "\n",
    "To read more about any of these topics, feel free to visit their corresponding documentation:\n",
    "* [GPU plugin](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_GPU.html)\n",
    "* [AUTO plugin](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_AUTO.html)\n",
    "* [MULTI device mode](https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Running_on_multiple_devices.html)\n",
    "* [Query Device Properties](https://docs.openvino.ai/latest/openvino_docs_OV_UG_query_api.html)\n",
    "* [Configurations for GPUs with OpenVINO](https://docs.openvino.ai/latest/openvino_docs_install_guides_configurations_for_intel_gpu.html)\n",
    "* [Benchmark python tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html)\n",
    "* [Asynchronous Inferencing](https://docs.openvino.ai/latest/openvino_docs_ie_plugin_dg_async_infer_request.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
