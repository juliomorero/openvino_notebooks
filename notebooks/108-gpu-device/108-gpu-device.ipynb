{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85bb873e-562a-4d60-9536-b6b88aecb5c2",
   "metadata": {},
   "source": [
    "# Working with GPUs in OpenVINO™\n",
    "\n",
    "This tutorial provides a high-level overview of working with Intel GPUs in OpenVINO. It shows users how to use Query Device to list system GPUs and check their properties, and it explains some of the key properties. It shows how to compile a model on GPU with performance hints and how to use multiple GPUs using MULTI or CUMULATIVE_THROUGHPUT. \n",
    "\n",
    "The tutorial shows example commands for benchmark_app that users can run to compare GPU performance in different configurations. It also provides code for a basic end-to-end application that compiles a model on GPU and uses it to run inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7253ed-d8a1-4475-8e83-263336639157",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f9f13-63a4-4b2d-95e3-9a82c1e9cebd",
   "metadata": {},
   "source": [
    "1. Background and context on how GPUs are used to speed up inference\n",
    "2. Introduce OpenVINO’s ability to run inference with GPUs\n",
    "3. How to configure OpenVINO to work with GPUs (link to Configuration for GPU with OpenVINO page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f35f17-5209-43d6-b3b5-db35efb1f42e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Checking GPUs with Query Device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3ce23-2b04-4014-9eeb-c2b8309eb103",
   "metadata": {},
   "source": [
    "1. List GPUs with ie.get_available_devices\n",
    "2. Check properties with ie.get_property\n",
    "3. Brief descriptions of key properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c37c2-b24b-4c9f-9ab8-d3a60b8393ec",
   "metadata": {},
   "source": [
    "In this section we will see how to list the available GPUs and check their properties. Some of the key properties will also be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e91abc-6118-4532-9817-6ef44c51b0c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### List GPUs with core.get_available_devices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b6152-3915-415a-987b-c30935462007",
   "metadata": {},
   "source": [
    "Firstly, in order to use GPUs, we must make sure our system is detecting them correctly.\n",
    "Running the following cell should output a list of compatible OpenVINO devices, in which our Intel GPUs should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8260bef-63c4-45f3-9ffd-4cc3ac892680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "core.available_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e266c79-7bb9-4907-9204-87b688087cbf",
   "metadata": {},
   "source": [
    "If the GPUs are installed correctly in the system and still don't appear in the list, we should follow the steps described [here](https://docs.openvino.ai/latest/openvino_docs_install_guides_configurations_for_intel_gpu.html) and try again. Once we have the GPUs working with OpenVINO we can proceed with the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0491c07-0b6d-483f-a4d6-d3f7b5ec0c81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check properties with core.get_property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54399a50-31cf-48bd-bf98-6dea0c0b1b20",
   "metadata": {},
   "source": [
    "Now, to get information and customize the behavior of our GPUs, we can use device properties. Devices in OpenVINO, such as CPUs and GPUs, have two types of properties: read-only and read-write. The former mainly shows information about the hardware itself like the device name or supported data types, while the latter allows us to tweak how the model is compiled, for instance to reduce latency or increase throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7509b6-67e3-46fb-bb23-5f46c58b0fc1",
   "metadata": {},
   "source": [
    "So, to get the value of a property, such as the device name, we can use the `core.get_property` method as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacdd5be-5d75-41d5-a51b-a376cb063b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "core.get_property(\"CPU\", \"FULL_DEVICE_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3129a-129f-49aa-aba0-71ae1e892ada",
   "metadata": {},
   "source": [
    "Each device also has a specific property, called `SUPPORTED_PROPERTIES`, that allows us to see all the available properties in the device (including the `SUPPORTED_PROPERTIES` itself). To do this, we repeat the above command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea1793-0784-40a1-a279-ca73a6994ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "core.get_property(\"CPU\", \"SUPPORTED_PROPERTIES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2d865-4223-4b50-804d-b4659e4984e2",
   "metadata": {},
   "source": [
    "Note that the value for each property has either a \"RO\" or \"RW\", which corresponds to the two types mentioned previously, \"**R**ead-**O**nly\" and \"**R**-**W**rite\" respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88546b71-2ae8-4519-b9b7-a54f444e204f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Brief descriptions of key properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524096f-cc77-4dca-9e52-e5a3c2c0523b",
   "metadata": {},
   "source": [
    "Each device has several properties as seen in the last command. Some of the key properties would be\n",
    "\n",
    "* Read-Only\n",
    "    * `FULL_DEVICE_NAME` - The product name of the GPU and whether it is an integrated or discrete GPU (iGPU or dGPU).\n",
    "    * `OPTIMIZATION_CAPABILITIES` - The model data types (INT8, FP16, FP32, etc) that are supported by this GPU.\n",
    "    * `GPU_EXECUTION_UNITS_COUNT` - The execution cores available in the GPU's architecture, which is a relative measure of the GPU's processing power.\n",
    "    * `RANGE_FOR_STREAMS` - The number of processing streams available on the GPU that can be used to execute parallel inference requests. When compiling a model in LATENCY or THROUGHPUT mode, OpenVINO will automatically select the best number of streams for low latency or high throughput.\n",
    "    * `DEVICE_GOPS` - The Giga operations per second count (GFLOPS) for each precision the device supports.\n",
    "* Read-Write\n",
    "    * `PERFORMANCE_HINT` - A high-level way to tune the device for a specific performance metric, such as latency or throughput, without worrying about device-specific settings.\n",
    "    * `INFERENCE_PRECISION_HINT` - A high-level way to specify which model data type to use for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a6b5d-623b-4893-8a6d-051ac670f288",
   "metadata": {},
   "source": [
    "To sum up this section, we can check the value for each property by simply looping through the dictionary returned by `core.get_property(\"GPU\", \"SUPPORTED_PROPERTIES\")` and then querying for that property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a913dc-8125-4270-9b3d-8e38c8324912",
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_properties = core.get_property(\"CPU\", \"SUPPORTED_PROPERTIES\")\n",
    "indent = len(max(supported_properties, key=len))\n",
    "for prop in supported_properties:\n",
    "    print(f\"{prop:>{indent}}:\", core.get_property(\"CPU\", prop))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131bd0f9-9b1e-4fc1-ad09-3960e1d6c50f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compiling a Model on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e33c10-f8cc-494d-bfa7-567787590946",
   "metadata": {},
   "source": [
    "1. Compile with default configuration (ie.compile_model(model, “GPU”)\n",
    "2. Throughput and latency performance hints\n",
    "3. Using multiple GPUs with multi-device and cumulative throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea15935-2373-4029-8bc0-998558a6defe",
   "metadata": {},
   "source": [
    "We now know what is a GPU, how to check if we have one and its properties but, how do we actually **use** one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc043aa-8729-45bc-9e98-6b3daea9f271",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile with default configuration (ie.compile_model(model, “GPU”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea18fb-7c2f-4809-a879-fe573c2a6d5b",
   "metadata": {},
   "source": [
    "In fact, due to the [AUTO plugin](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_AUTO.html#how-auto-works), we might already be using GPUs if they are properly recognized by OpenVINO. Despite this, if we want to use a specific device, we can do so by compiling our models as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7d00e-ebe5-456d-8643-112e9d8c860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load any model\n",
    "model = core.read_model(model=\"../001-hello-world/model/v3-small_224_1.0_float.xml\")\n",
    "\n",
    "# compile the loaded model using the GPU device\n",
    "compiled_model = core.compile_model(model, \"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f128a-4d6b-4d16-b72e-ff5fe54e9480",
   "metadata": {},
   "source": [
    "Note that above we are using `\"GPU\"` which is an alias for `\"GPU.0\"` according to the [docs](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_GPU.html). Actually, as expected, any of the values returned by `core.available_devices` are valid device specifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cbc6cd-fca3-4d7b-9530-71893e68a57a",
   "metadata": {},
   "source": [
    "Once we have a compiled model, we can check for its properties, just as we did with the GPU devices in the previous section. Instead of calling `core.get_property` with a specific device, we use `compiled_model.get_property` directly with the property name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc90ead-493f-4f0d-a4d9-c53031fac2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model.get_property(\"SUPPORTED_PROPERTIES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0a2d6-5294-4e1d-b3f9-ac4b327315e7",
   "metadata": {},
   "source": [
    "As we can appreciate from the above result, the `PERFORMANCE_HINT` property defined earlier reappeared here, let's take a deeper look at it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa916d7-4626-4f54-9b34-954eca19aafb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Throughput and latency performance hints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb73533-2767-4816-9c7f-83487c69bdfa",
   "metadata": {},
   "source": [
    "Essentially, the `PERFORMANCE_HINT` allows us to easily tweak either our device or model properties to better suit certain tasks. Currently, it supports three values: `\"LATENCY\"` which prioritizes short response times for each inference, `\"THROUGHPUT\"` which helps inferring large amounts of data at the same time like a video feed, and `\"CUMULATIVE_THROUGHPUT\"` which will see later. The hint's behavior is the same in both the device and in the model, except for the fact that in the device the hint can be considered a \"global\" property, i.e all models created on that device will use the same value of the hint, whereas in the compiled model acts as a \"local\" one, i.e only affects that model. See [the docs](https://docs.openvino.ai/latest/openvino_docs_OV_UG_query_api.html#setting-properties-globally) for more info.\n",
    "\n",
    "Until now we have only queried either the device or the compiled model for properties so, how can we modify those that are read-write such as the `PERFOMANCE_HINT`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11acfc8-909b-4efc-816e-6df0d96b7743",
   "metadata": {},
   "source": [
    "To modify properties in the device, we can use the `core.set_property` method, in which instead of just using the property name, a dictionary is required specifying the value as well. For example, if we want to change the `PERFORMANCE_HINT` to improve latency we can do it like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b488c-5128-4dea-a795-9d2ad71dca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "core.set_property(\"CPU\", {\"PERFORMANCE_HINT\": \"LATENCY\"})\n",
    "core.get_property(\"CPU\", \"PERFORMANCE_HINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a9b18-4e6d-445a-929e-a47f9f2bee00",
   "metadata": {},
   "source": [
    "If instead we only want to affect a certain model, we need to add the dictionary as the third argument in the `core.compile_model` method. For instance, to improve throughtput for just this model the following works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5407ef85-32b7-47b0-835c-8ef395a6e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model, \"CPU\", {\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "compiled_model.get_property(\"PERFORMANCE_HINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a98815-669e-4868-a5f3-7104d6887fb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using multiple GPUs with multi-device and cumulative throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da18381-edfc-4d3b-942a-87d6b0cbd5d3",
   "metadata": {},
   "source": [
    "The latency and throughput hints mentioned above are great and can make a difference when used adequately but they usually use just one device, either due to the [AUTO plugin](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_AUTO.html#how-auto-works) or by manual specification of the device name as we did above. In case we have multiple devices, such as an integrated and discrete GPU, we could use both at the same time to improve the utlization of our resources. In order to do this, OpenVINO provides a virtual device called [MULTI](https://docs.openvino.ai/nightly/openvino_docs_OV_UG_Running_on_multiple_devices.html), which is just a combination of our existent devices that knows how to split inference work between them, leveraging the capabilities of each device.\n",
    "\n",
    "So, as an example, if we want to use both our integrated and discrete GPUs and the CPU at the same time, we can compile our model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029e7910-70b8-4971-8b82-0569accbc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model=model, device_name=\"MULTI:GPU.1,GPU.0,CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53791dad-88a5-4198-9013-5ebd2e950d52",
   "metadata": {},
   "source": [
    "Note that we always need to explicitly specify the device list for MULTI to work, as otherwise MULTI does not know which devices are available for inference. However, this is not the only way to use multiple devices in OpenVINO. There is another `PERFORMANCE_HINT` called CUMULATIVE_THROUGHPUT that works similar to MULTI, except it uses the devices automatically selected by AUTO. This way, we don't need to manually specify which devices to use. Here's an example showing how to use CUMULATIVE_THROUGHPUT, equivalent to the MULTI one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134705f4-0e8f-40d1-b9fa-26dfbcc69c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model, \"AUTO\", {\"PERFORMANCE_HINT\": \"CUMULATIVE_THROUGHPUT\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99351910-c54c-4f55-b1ce-eb74125a9dbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance Comparison with benchmark_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f453eca-41ac-42fc-b77b-9aa8d8e3ab20",
   "metadata": {},
   "source": [
    "1. Commands showing users how to run benchmark_app on GPU with various performance hints\n",
    "2. Show performance results with a basic model (person-detection-0303, perhaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b411e5-5c56-462a-82e3-22ba020f64c1",
   "metadata": {},
   "source": [
    "Given all the different options available when compiling a model, it may be difficult to know which settings work best for a certain application. Thankfully, OpenVINO provides a performance benchmarking tool called `benchmark_app`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df239f8c-2597-46d4-86a0-c23bd1591a33",
   "metadata": {},
   "source": [
    "### Commands showing users how to run benchmark_app on GPU with various performance hints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5921100-0f41-42d3-8f90-703c20cb11bd",
   "metadata": {},
   "source": [
    "The basic syntax of benchmark_app is as follows:\n",
    "\n",
    "```bash\n",
    "benchmark_app -m PATH_TO_MODEL -d TARGET_DEVICE -hint {throughput,cumulative_throughput,latency,none}\n",
    "```\n",
    "where TARGET_DEVICE is any device shown by `core.available_devices` as well as the MULTI and AUTO devices we saw previously, and the value of hint should be one of the values between brackets. \n",
    "\n",
    "Note that benchmark_app only requires the model path to run but both the device and hint arguments will be useful to us. For more advanced usages, the tool itself has other options that can be checked by running `benchmark_app -h` or reading the [docs](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f35f7-3592-4b48-ae54-533fd958a35c",
   "metadata": {},
   "source": [
    "So, as an example, if we want to benchmark a simple model using a GPU with a latency focus, we would run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c76637-f86c-41d5-8846-6e3946126084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 21.41 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input:0 (node: input) : f32 / [...] / [1,224,224,3]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     MobilenetV3/Predictions/Softmax:0 (node: MobilenetV3/Predictions/Softmax) : f32 / [...] / [1,1001]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input:0 (node: input) : u8 / [N,H,W,C] / [1,224,224,3]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     MobilenetV3/Predictions/Softmax:0 (node: MobilenetV3/Predictions/Softmax) : f32 / [...] / [1,1001]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 155.27 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: v3-small_224_1.0_float\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1\n",
      "[ INFO ]   NUM_STREAMS: 1\n",
      "[ INFO ]   AFFINITY: Affinity.NONE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 12\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input:0'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input:0' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 1 inference requests, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 3.26 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            34635 iterations\n",
      "[ INFO ] Duration:         60002.23 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        1.54 ms\n",
      "[ INFO ]    Average:       1.69 ms\n",
      "[ INFO ]    Min:           1.15 ms\n",
      "[ INFO ]    Max:           32.01 ms\n",
      "[ INFO ] Throughput:   577.23 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m model/v3-small_224_1.0_float.xml -d CPU -hint latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81464f73-ef33-47fa-9239-7a817a1ce35b",
   "metadata": {},
   "source": [
    "### Show performance results with a basic model (person-detection-0303, perhaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89e723-0ad4-4e0f-987c-c1ec52628271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"benchmark_report.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd1496-996d-49d8-80b9-3bfb3dda9729",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Application Using GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bacb918-3668-4e72-92ba-a15db658c369",
   "metadata": {},
   "source": [
    "1. Provide end-to-end sample code for running inference on GPU in a basic application\n",
    "\n",
    "We will now show an end-to-end object detection example running on GPU.\n",
    "\n",
    "    1. Import necessary packages\n",
    "    2. Download and convert ssdlite-mobilenet-v2 model\n",
    "    3. Read ssd-mobilenet model and compile model on GPU in THROUGHPUT mode\n",
    "    4. Load every frame of a video and resize it to shape expected by model\n",
    "    5. Set up AsyncInferQueue, process every frame of video and store results, time how long it takes\n",
    "    6. Post-process results to print every object detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1686090-2a2e-4ae6-8f22-7ef458ce221b",
   "metadata": {},
   "source": [
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "55e0b948-ec1f-4ab8-9418-103dc8f06c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPU']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, Video, display\n",
    "from openvino.runtime import AsyncInferQueue, CompiledModel, Core, InferRequest\n",
    "\n",
    "core = Core()\n",
    "core.available_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24a39d-8819-47a3-9c91-0f80e92649b6",
   "metadata": {},
   "source": [
    "### Download the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "061a741f-2fc7-430e-b2f7-21d229533447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A directory where the model will be downloaded.\n",
    "base_model_dir = \"model\"\n",
    "\n",
    "# The name of the model from Open Model Zoo\n",
    "model_name = \"ssdlite_mobilenet_v2\"\n",
    "\n",
    "model_path = f\"model/public/{model_name}\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    download_command = f\"omz_downloader \" \\\n",
    "                       f\"--name {model_name} \" \\\n",
    "                       f\"--output_dir {base_model_dir} \" \\\n",
    "                       f\"--cache_dir {base_model_dir}\"\n",
    "    ! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaa8d2-724b-47f5-991d-22bbe6d1c994",
   "metadata": {},
   "source": [
    "### Convert the Model to OpenVINO IR format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd75b02b-d0a1-422f-b3db-42558ccf5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = \"FP16\"\n",
    "\n",
    "# The output path for the conversion.\n",
    "converted_model_path = f\"model/public/{model_name}/{precision}/{model_name}.xml\"\n",
    "\n",
    "if not os.path.exists(converted_model_path):\n",
    "    convert_command = f\"omz_converter \" \\\n",
    "                      f\"--name {model_name} \" \\\n",
    "                      f\"--download_dir {base_model_dir} \" \\\n",
    "                      f\"--precisions {precision}\"\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfb30e-d6d3-4ed3-8e7e-f1a843ce2ec0",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8648ae4f-2c73-4525-ace0-1c71afb18ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<ConstOutput: names[image_tensor, image_tensor:0] shape[1,300,300,3] type: u8>]\n",
      "Model input shape: 1 300 300 3\n"
     ]
    }
   ],
   "source": [
    "model = core.read_model(model=converted_model_path)\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\", config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "\n",
    "# Get the input and output nodes.\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "input_keys = list(compiled_model.inputs)\n",
    "print(input_keys)\n",
    "\n",
    "# Get the input size.\n",
    "num, height, width, channels = input_layer.shape\n",
    "print('Model input shape:', num, height, width, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7003286a-8349-46f6-9d38-c6bb959e892a",
   "metadata": {},
   "source": [
    "### Define Model Output Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "621b919e-c120-42f9-89eb-ffd7b810c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO classes! (the dataset, not the dog)\n",
    "classes = [\n",
    "    \"background\", \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\",\n",
    "    \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"street sign\", \"stop sign\",\n",
    "    \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\",\n",
    "    \"bear\", \"zebra\", \"giraffe\", \"hat\", \"backpack\", \"umbrella\", \"shoe\", \"eye glasses\",\n",
    "    \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\",\n",
    "    \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\",\n",
    "    \"plate\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
    "    \"couch\", \"potted plant\", \"bed\", \"mirror\", \"dining table\", \"window\", \"desk\", \"toilet\",\n",
    "    \"door\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\",\n",
    "    \"toaster\", \"sink\", \"refrigerator\", \"blender\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "    \"teddy bear\", \"hair drier\", \"toothbrush\", \"hair brush\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bce3d-647d-46d1-8540-4a2935bc829f",
   "metadata": {},
   "source": [
    "### Load and Preprocess Video Frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fcee85d9-eab1-4a94-bb2f-d5217fd7cdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading video...\n",
      "Video loaded!\n",
      "Frame shape:  (1, 300, 300, 3)\n",
      "Number of frames:  288\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../data/video/Coco Walking in Berkeley.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load video\n",
    "video_file = \"../data/video/Coco Walking in Berkeley.mp4\"\n",
    "video = cv2.VideoCapture(video_file)\n",
    "framebuf = []\n",
    "\n",
    "print('Loading video...')\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        print('Video loaded!')\n",
    "        video.release()\n",
    "        break\n",
    "    \n",
    "    # Preprocess frames - convert them to shape expected by model\n",
    "    input_frame = cv2.resize(src=frame, dsize=(width, height), interpolation=cv2.INTER_AREA)\n",
    "    input_frame = np.expand_dims(input_frame, axis=0)\n",
    "\n",
    "    # Append frame to framebuffer\n",
    "    framebuf.append(input_frame)\n",
    "    \n",
    "\n",
    "print('Frame shape: ', framebuf[0].shape)\n",
    "print('Number of frames: ', len(framebuf))\n",
    "\n",
    "# Show original video file\n",
    "Video(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad10b97-e32e-4885-af1d-0636e970b707",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set up Asynchronous Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6ed58-8b3f-415e-a733-a7517e200b12",
   "metadata": {},
   "source": [
    "#### Callback Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1268e1a4-b530-4b39-b79d-d6d5f988a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_callback(infer_request: InferRequest, results) -> None:\n",
    "    predictions = next(iter(infer_request.results.values()))\n",
    "    results.append(predictions[:10]) # Grab first 10 predictions for this frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f6aba-b3ba-478b-b02f-6b438b575c33",
   "metadata": {},
   "source": [
    "#### Create Async Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56e8eae2-20a9-43dc-8774-444ad0179136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create async queue with optimal number of infer requests\n",
    "infer_queue = AsyncInferQueue(compiled_model)\n",
    "infer_queue.set_callback(completion_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47ae59-b86d-46c4-b219-e7beccd5f796",
   "metadata": {},
   "source": [
    "### Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "59f493dd-4cb7-4ae2-b117-7ba41a950d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to inference all frames: 1.863s\n",
      "Time per frame: 0.006467s (154.623 FPS)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "start_time = time.time()\n",
    "for i, input_frame in enumerate(framebuf):\n",
    "    infer_queue.start_async({0: input_frame}, results)\n",
    "\n",
    "infer_queue.wait_all()\n",
    "stop_time = time.time()\n",
    "\n",
    "total_time = stop_time - start_time\n",
    "time_per_frame = total_time / len(framebuf)\n",
    "fps = len(framebuf) / total_time\n",
    "print(f'Total time to inference all frames: {total_time:.3f}s')\n",
    "print(f'Time per frame: {time_per_frame:.6f}s ({fps:.3f} FPS)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd90b68-ae14-4fe9-98c7-e8d23146c82c",
   "metadata": {},
   "source": [
    "### Process Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1ccba843-e40b-43a8-b2b4-8ebd67bc1424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video loaded!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"output.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set minimum detection threshold\n",
    "min_thresh = .6\n",
    "\n",
    "# Load video\n",
    "video = cv2.VideoCapture(video_file)\n",
    "\n",
    "# Get video parameters\n",
    "frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "fourcc = int(video.get(cv2.CAP_PROP_FOURCC))\n",
    "\n",
    "output = cv2.VideoWriter('output.mp4', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while video.isOpened():\n",
    "    current_frame = int(video.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        print('Video loaded!')\n",
    "        output.release()\n",
    "        video.release()\n",
    "        break\n",
    "\n",
    "    # prediction contains [image_id, label, conf, x_min, y_min, x_max, y_max] according to model\n",
    "    for prediction in np.squeeze(results[current_frame]):\n",
    "        if prediction[2] > min_thresh:\n",
    "            x_min = int(prediction[3] * frame_width)\n",
    "            y_min = int(prediction[4] * frame_height)\n",
    "            x_max = int(prediction[5] * frame_width)\n",
    "            y_max = int(prediction[6] * frame_height)\n",
    "            label = classes[int(prediction[1])]\n",
    "            \n",
    "            # Draw a bounding box with its label above it\n",
    "            image = cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0,255,0), 2)\n",
    "            cv2.putText(image, label, (x_min, y_min - 10), cv2.FONT_ITALIC, 1, (255,0,0), 2)\n",
    "\n",
    "    output.write(frame)\n",
    "\n",
    "# Show output\n",
    "Video(\"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb6bb6-0d1c-4b0b-8a0b-efb5667816ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb4f28-2917-45ca-8fa0-8c670c0348bc",
   "metadata": {},
   "source": [
    "1. GPUs are easy to use with OpenVINO and considerably boost performance\n",
    "2. Links to OpenVINO documentation where readers can learn more"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
